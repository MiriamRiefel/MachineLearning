{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Lab Assignment 2: Evaluate classifiers (10 points)\n",
    " \n",
    "In this assignment you will optimize and compare the perfomance of a parametric (logistic regression) and non-parametric (k-nearest neighbours) classifier on the MNIST dataset.\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline.  \n",
    "\n",
    "### Deadline Friday, November 17, 23:59.\n",
    "\n",
    "This notebook consists of three parts: design, implementation, results & analysis. \n",
    "We provide you with the design of the experiment and you have to implement it and analyse the results.\n",
    "\n",
    "### Criteria used for grading\n",
    "* Explain and analyse all results.\n",
    "* Make your notebook easy to read. When you are finished take your time to review it!\n",
    "* You do not want to repeat the same chunks of code multiply times. If your need to do so, write a function. \n",
    "* The implementation part of this assignment needs careful design before you start coding. You could start by writing pseudocode.\n",
    "* In this exercise the insights are important. Do not hide them somewhere in the comments in the implementation, but put them in the Analysis part\n",
    "* Take care that all the figures and tables are well labeled and numbered so that you can easily refer to them.\n",
    "* A plot should have a title and axes labels.\n",
    "* You may find that not everything is 100% specified in this assignment. That is correct! Like in real life you probably have to make some choices. Motivate your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading points distribution\n",
    "\n",
    "* Implementation 5 points\n",
    "* Results and analysis 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to keep the order of this design and are allowed to alter it if you are confident.\n",
    "* Import all necessary modules. Try to use as much of the available functions as possible. \n",
    "* Use the provided train and test set of MNIST dataset.\n",
    "* Pre-process data eg. normalize/standardize, reformat, etc.           \n",
    "  Do whatever you think is necessary and motivate your choices.\n",
    "* (1) Train logistic regression and k-nn using default settings.\n",
    "* Use 10-fold cross validation for each classifier to optimize the performance for one parameter: \n",
    "    * consult the documentation on how cross validation works in sklearn (important functions:             cross_val_score(), GridSearchCV()).\n",
    "    * Optimize k for k-nn,\n",
    "    * for logistic regression focus on the regularization parameter,\n",
    "* (2) Train logistic regression and k-nn using optimized parameters.\n",
    "* Show performance on the cross-validation set for (1) and (2) for both classifiers: \n",
    "    * report the average cross validation error rates (alternatively, the average accuracies - it's up to you) and standard deviation,\n",
    "    * plot the average cross valildation errors (or accuracies) for different values of the parameter that you tuned. \n",
    "* Compare performance on the test set for two classifiers:\n",
    "    * produce the classification report for both classifiers, consisting of precision, recall, f1-score. Explain and analyse the results.\n",
    "    * print confusion matrix for both classifiers and compare whether they missclassify the same  classes. Explain and analyse the results.\n",
    "* Discuss your results.\n",
    "* BONUS: only continue with this part if you are confident that your implemention is complete \n",
    "    * tune more parameters of logistic regression\n",
    "    * add additional classifiers (NN, Naive Bayes, decision tree), \n",
    "    * analyse additional dataset (ex. Iris dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "# importing learning functions\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression classifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV #to tune parameters using cross-validation\n",
    "from scipy.spatial import distance #to calculate the Euclidean distance\n",
    "from collections import Counter #to count unique occurances of items in array, for majority voting\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN classifier\n",
    "from sklearn.datasets import load_digits # the dataset\n",
    "\n",
    "# load mnist dataset and split in train and test set.\n",
    "digits = load_digits()\n",
    "x_train_mnist = reshape(digits.images[:1500],(1500,64))\n",
    "x_test_mnist = reshape(digits.images[1500:],(297,64))\n",
    "y_train_mnist = digits.target[:1500]\n",
    "y_test_mnist = digits.target[1500:]\n",
    "\n",
    "# your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 64)\n",
      "(297, 64)\n"
     ]
    }
   ],
   "source": [
    "# I first want to see what my data looks like\n",
    "print(shape(x_train_mnist))\n",
    "print(shape(x_test_mnist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalisation of the input, in order to prevent biasing the learning process. \n",
    "# I have made a general function, because this needs to happen for the train and test set\n",
    "def standard(x):\n",
    "    x = x - mean(x)\n",
    "    x = x/ std(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8135586  -0.8135586   0.01971206 ..., -0.8135586  -0.8135586\n",
      "  -0.8135586 ]\n",
      " [-0.8135586  -0.8135586  -0.8135586  ...,  0.85298272 -0.8135586\n",
      "  -0.8135586 ]\n",
      " [-0.8135586  -0.8135586  -0.8135586  ...,  1.85290751  0.68632859\n",
      "  -0.8135586 ]\n",
      " ..., \n",
      " [-0.8135586  -0.8135586  -0.8135586  ...,  0.68632859 -0.8135586\n",
      "  -0.8135586 ]\n",
      " [-0.8135586  -0.64690447  0.68632859 ...,  0.68632859 -0.64690447\n",
      "  -0.8135586 ]\n",
      " [-0.8135586   0.01971206  1.85290751 ...,  0.51967445  0.18636619\n",
      "  -0.8135586 ]]\n"
     ]
    }
   ],
   "source": [
    "# normalisation\n",
    "x_train_mnist = standard(x_train_mnist)\n",
    "x_test_mnist = standard(x_test_mnist)\n",
    "# I want to see whether it outputs what I want\n",
    "print(x_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression: default parameters\n",
    "LR = LogisticRegression()\n",
    "print(LR.fit(x_train_mnist, y_train_mnist, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traininset prediction scores are:\n",
      "0.988666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"traininset prediction scores are:\")\n",
    "print(LR.score(x_train_mnist, y_train_mnist, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x000001821B6BC150>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split my training set into 10 folds, in order to get a better fit in classifcation\n",
    "k_fold = KFold(n_splits=10)\n",
    "k_fold.split(x_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing logistic regression over different values of C: regularisatoin parameter\n",
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100] }\n",
    "grid = GridSearchCV(estimator=LR, param_grid=param_grid)\n",
    "grid.fit(x_train_mnist, y_train_mnist)\n",
    "clf = GridSearchCV(LR, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "# I want to find the best parameter, so that the model will fit the data well. \n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbor: default setting with 2 nearest neighbors\n",
    "KNN = KNeighborsClassifier(n_neighbors=2)\n",
    "print(KNN.fit(x_train_mnist, y_train_mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN traininset prediction scores are:\n",
      "0.991333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN trainingset prediction scores are:\")\n",
    "print(KNN.score(x_train_mnist, y_train_mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing k-nearest neighbor over different values of k: the number of neighbors\n",
    "param_grid = { 'n_neighbors' : [1,2,3,4,5,6]}\n",
    "grid = GridSearchCV(estimator=KNN, param_grid=param_grid)\n",
    "grid.fit(x_train_mnist, y_train_mnist)\n",
    "clf = GridSearchCV(KNN, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960666666667\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "# I want to find the best parameter, so that the model will fit the data well.\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and analysis of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now I will plot accuracies for different values of K in my CV set for KNN classification\n",
    "def accuracies():\n",
    "    score = np.array([])\n",
    "    for k in np.array([1,2,3,4,5,6]):\n",
    "        KNN = KNeighborsClassifier(n_neighbors=k)\n",
    "        KNN.fit(x_train_mnist, y_train_mnist)\n",
    "        score = np.append(score, KNN.score(x_train_mnist,y_train_mnist,sample_weight=None))\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEjCAYAAAAYFIcqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1821d13bdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1, 2, 3, 4, 5, 6], accuracies(), 'ro')\n",
    "plt.axis([0, 7, 0.98, 1.01])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.suptitle('Accuracy scores K-Nearest Neighbor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the average cross valildation errors (or accuracies) for different values of the parameter that you used\n",
    "def accuraciesLR():\n",
    "    score = np.array([])\n",
    "    for c in np.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100]):\n",
    "        LR = LogisticRegression(C=c)\n",
    "        LR.fit(x_train_mnist, y_train_mnist)\n",
    "        score = np.append(score, LR.score(x_train_mnist,y_train_mnist,sample_weight=None))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEjCAYAAAAhczZxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZkWcMMzMrMj/B1JloZwx/phDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1821878ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([1,2,3,4,5,6,7], accuraciesLR(), 'ro')\n",
    "plt.xlabel('0.0001        0.001       0.01          0.1          1             10           100')\n",
    "plt.ylabel('accuracy')\n",
    "plt.suptitle('Accuracy scores logistic Regression, dependent on C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LR trainingset prediction scores using optimised C are:\n",
      "0.988666666667\n",
      "LR testset prediction scores using optimised C are:\n",
      "0.895622895623\n"
     ]
    }
   ],
   "source": [
    "# Using the optimal parameter to create a new model with LR. \n",
    "LR = LogisticRegression(C=1)\n",
    "print(LR.fit(x_train_mnist, y_train_mnist, sample_weight=None))\n",
    "print(\"LR trainingset prediction scores using optimised C are:\")\n",
    "print(LR.score(x_train_mnist, y_train_mnist, sample_weight=None))\n",
    "# I also want to use this optimised model on the test set, to do a final test of my LR model\n",
    "print(\"LR testset prediction scores using optimised C are:\")\n",
    "print(LR.score(x_test_mnist, y_test_mnist, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n",
      "KNN trainingset prediction scores using optimised k are:\n",
      "1.0\n",
      "KNN testset prediction scores using optimised k are:\n",
      "0.942760942761\n"
     ]
    }
   ],
   "source": [
    "# Using the optimal parameter to create a new model with kNN. \n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "print(KNN.fit(x_train_mnist, y_train_mnist))\n",
    "print(\"KNN trainingset prediction scores using optimised k are:\")\n",
    "print(KNN.score(x_train_mnist, y_train_mnist, sample_weight=None))\n",
    "# I also want to use this optimised model on the test set, to do a final test of my KNN model\n",
    "print(\"KNN testset prediction scores using optimised k are:\")\n",
    "print(KNN.score(x_test_mnist, y_test_mnist, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NearestNeighbor scores: \n",
      "precision = 0.945756665325\n",
      "recall = 0.943612623935\n",
      "fscore = 0.942691278818\n",
      "\n",
      "Logistic Regression scores: \n",
      "precision = 0.900340706582\n",
      "recall = 0.897804465546\n",
      "fscore = 0.89570003054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y_test_mnist\n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN.fit(x_train_mnist, y_train_mnist)\n",
    "y_pred = KNN.predict(x_test_mnist)\n",
    "score = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "print(\"k-NearestNeighbor scores: \")\n",
    "print(\"precision =\" , average(score[0]))\n",
    "print(\"recall =\" , average(score[1]))\n",
    "print(\"fscore =\" , average(score[2]))\n",
    "print(\"\")\n",
    "\n",
    "LR = LogisticRegression(C=1)\n",
    "LR.fit(x_train_mnist, y_train_mnist, sample_weight=None)\n",
    "y_pred = LR.predict(x_test_mnist)\n",
    "score = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "print(\"Logistic Regression scores: \")\n",
    "print(\"precision =\" , average(score[0]))\n",
    "print(\"recall =\" , average(score[1]))\n",
    "print(\"fscore =\" , average(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this experiment, I have prepared the data through sorting they training- and testsets and regularising their values. The initial classification tasks had high prediction scores (LR = 0.99, KNN = 0.99) This was mainly because the model was tested on the same data it was trained on. After the data set was split into 10 folds, using the k_fold method, the models are less prone to overfitting, because it is using a cross-validation method and thus not testing on the same data as trained on. This explains why, even after some parameters were optimised, the prediction scores for both LR and k-NN were lower (LR = 0.93, KNN = 0.96). These were, although more realistic than the initial scores, still based on the training/cross-validation set. I visualised the cross-validation process for both classification methods in graphs. Then I tested the models on the testset. This yielded a prediction score of 0.90 for LR and 0.94 for KNN. \n",
    "\n",
    "In the overview we can see that for the k-NN model, precision, recall and fscore all approximately have the same value. Precision is the proportion of true positives to false positives. Recall is the proportion of true positives to false negatives. This means that the number of false positives and false negatives is approximately the same. The f-score gives the weighted harmonic mean of the precision and the proportion. \n",
    "\n",
    "For the LR model, precision is slightly higher than the recall. This means that the number of false positives is slightly higher than false negatives. \n",
    "\n",
    "The k-Nearest Neighbor classification model performs better on this data. All test scores are higher than the test scores of Logistic Regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
